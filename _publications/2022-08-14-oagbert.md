---
title: "OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services"
collection: publications
permalink: /publication/2022-08-14-oagbert
excerpt: 'This paper is about training large language model on top of scientific data.'
date: 2022-08-14
venue: 'The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining'
paperurl: 'http://keg.cs.tsinghua.edu.cn/jietang/publications/KDD22-Liu-et-al-OAG-BERT.pdf'
citation: 'Liu, Xiao & Yin, Da & Zheng, Jingnan & Zhang, Xingjian & Zhang, Peng & Yang, Hongxia & Yuxiao, Dong & Tang, Jie. (2022). OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services. 3418-3428. 10.1145/3534678.3539210.'
---

Academic knowledge services have substantially facilitated the development of the science enterprise by providing a plenitude of efficient research tools. However, many applications highly depend on ad-hoc models and expensive human labeling to understand scientific contents, hindering deployments into real products. To build a unified backbone language model for different knowledge-intensive academic applications, we pre-train an academic language model OAG-BERT that integrates both the heterogeneous entity knowledge and scientific corpora in the Open Academic Graph (OAG) -- the largest public academic graph to date. In OAG-BERT, we develop strategies for pre-training text and entity data along with zero-shot inference techniques. In OAG-BERT, we develop strategies for pre-training text and entity data along with zero-shot inference techniques. Its zero-shot capability furthers the path to mitigate the need of expensive annotations. OAG-BERT has been deployed for real-world applications, such as the reviewer recommendation function for National Nature Science Foundation of China (NSFC) -- one of the largest funding agencies in China -- and paper tagging in AMiner. All codes and pre-trained models are available via the CogDL toolkit.

[Download paper here](http://keg.cs.tsinghua.edu.cn/jietang/publications/KDD22-Liu-et-al-OAG-BERT.pdf)

Recommended citation: Liu, Xiao & Yin, Da & Zheng, Jingnan & Zhang, Xingjian & Zhang, Peng & Yang, Hongxia & Yuxiao, Dong & Tang, Jie. (2022). OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services. 3418-3428. 10.1145/3534678.3539210.